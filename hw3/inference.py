# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UkD0vXZB0hvXTJVFbD1tgh3Ed86_SnaL
"""
import os
from os.path import exists, join, isdir
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig
from peft import PeftModel
from peft.tuners.lora import LoraLayer
import argparse
from utils import get_prompt, get_bnb_config
import json

parser = argparse.ArgumentParser(description="convert test.json into correct form")
parser.add_argument(
    "--model_path",
    type=str,
    default=None,
    help="The name of the test.json to use (via the datasets library).",
)
parser.add_argument(
    "--adapter_path",
    type=str,
    default=None,
    help="The name of the test.json to use (via the datasets library).",
)
parser.add_argument(
    "--test_data",
    type=str,
    default=None,
    help="The name of the test.json to use (via the datasets library).",
)
parser.add_argument(
    "--output_path",
    type=str,
    default="prediction.json",
    help="The name of the test.json to use (via the datasets library).",
)
args = parser.parse_args()

def get_last_checkpoint(checkpoint_dir):
    if isdir(checkpoint_dir):
        is_completed = exists(join(checkpoint_dir, 'completed'))
        if is_completed: return None, True # already finished
        max_step = 0
        for filename in os.listdir(checkpoint_dir):
            if isdir(join(checkpoint_dir, filename)) and filename.startswith('checkpoint'):
                max_step = max(max_step, int(filename.replace('checkpoint-', '')))
        if max_step == 0: return None, is_completed # training started, but no checkpoint
        checkpoint_dir = join(checkpoint_dir, f'checkpoint-{max_step}')
        print(f"Found a previous checkpoint at: {checkpoint_dir}")
        return checkpoint_dir, is_completed # checkpoint found!
    return None, False # first training


max_new_tokens = 256
top_p = 0.6
temperature=0.6
user_question = "What is Einstein's theory of relativity?"

# Base model
model_name_or_path = args.model_path
# Adapter name on HF hub or local checkpoint path.
#adapter_path, _ = get_last_checkpoint('/content/drive/MyDrive/Laurence_2/model_backup')
adapter_path = args.adapter_path

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
# Fixing some of the early LLaMA HF conversion issues.
tokenizer.bos_token_id = 1
tokenizer.eos_token_id = 2
tokenizer.unk_token_id = 4

# Load the model (use bf16 for faster inference)
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    torch_dtype=torch.bfloat16,
    device_map={"": 0},
    load_in_4bit=True,
    quantization_config=get_bnb_config()
)

model = PeftModel.from_pretrained(model, adapter_path)
model.eval()
'''
prompt = (
    "你是人工智慧助理，以下是用戶和人工智能助理之間的對話。你要對用戶的問題提供有用、安全、詳細和禮貌的回答。\n###USER: {user_question} \n###ASSISTANT:"
)
'''
def generate(model, user_question, max_new_tokens=max_new_tokens, top_p=top_p, temperature=temperature):
    inputs = tokenizer(get_prompt(user_question), return_tensors="pt").to('cuda')

    outputs = model.generate(
        **inputs,
        generation_config=GenerationConfig(
            do_sample=True,
            max_length=1024,
            max_new_tokens = max_new_tokens,
            top_p=top_p,
            temperature=temperature,
        )
    )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(text)
    return text

with open(args.test_data, encoding="utf-8") as f:
  data = json.load(f)
out = []
for i in data:
  user_question = i['instruction']
  print(user_question)
  tmp = {}
  tmp["id"] = i['id']
  ans = generate(model, user_question)
  ans = ans.split("ASSISTANT: ")
  print("---------\n" + ans[1] +"\n----------")
  tmp["output"] = ans[1]
  out.append(tmp)
  #import pdb; pdb.set_trace()

print(out)
with open(args.output_path, 'w', encoding="utf-8") as outfile:
  json.dump(out, outfile, indent = 2 ,ensure_ascii=False)

